# **Preservation of the Global Knowledge byNot-True Distillation in Federated Learning**

（***\*通过联邦学习中的非真实蒸馏来保存全局知识\****）



摘要：

>问题：但全局模型的融合通常会受到<font color='blue'>数据异构性</font>的影响
>本文：
>
>发现**遗忘**可能是联邦学习的瓶颈。我们观察到全局模型忘记了前几轮的知识，而局部训练导致忘记了局部分布之外的知识。
>
>根据我们的发现，我们假设解决遗忘问题将缓解数据异构性问题
>
>提出： **Federated Not-True Distillation(FedNTD)**
>
>它为**本地可用的**  非正确类别 **的数据** 保留全局视角
>
>在实验中，FedNTD 在各种设置上展示了最先进的性能，而不会损害数据隐私或产生额外的通信成本



1、简介

>①介绍联邦学习
>
>②介绍Fedavg ,引入Non-iid 数据异构的问题
>
>③介绍数据异构性 和解决的异构性的好处
>
>
>
>![image-20221204190648206](C:\Users\bai\AppData\Roaming\Typora\typora-user-images\image-20221204190648206.png)
>
>④持续学习：由于模型在一系列任务上不断更新，会导致，为了适应新任务，而忘掉先前任务中的参数。造成严重的遗忘。
>
>⑤猜想联邦学习同样存在着类似持续学习的遗忘
>
>⑥**观察结果验证了我们的猜想：联邦学习会遗忘**。全局模型的预测在通信轮次之间高度不一致，显着降低了先前模型原本预测良好的某些类别的预测性能，对应于局部分布之外的区域的全局知识很容易被遗忘。由于仅对本地模型进行平均无法恢复它，因此全局模型难以保留先前的知识。
>
>⑦我们假设减轻遗忘问题可以缓解数据异质性
>
>FedNTD 利用全局模型对本地可用数据的预测，但仅针对不正确的类别
>
>我们证明了 FedNTD 对保存本地分布之外全局知识的影响及其对联邦学习的好处。
>
>
>
>贡献：
>
>①研究遗忘，并且遗忘与数据异构有关  （二节）
>
>②提出FedNTD (三节)（四节）
>
>③分析FedNTD 如何让联邦学习收益   （五节）








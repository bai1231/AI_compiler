# **NeuLens: Spatial-based Dynamic Acceleration of Convolutional  Neural Networks on Edge**

基于空间的边缘卷积神经网络的动态加速

### 0、摘要：

><font color='blue'>问题:</font>
>
>最先进的CNN加速的难点： 在一般计算平台上限的实际延迟加速  和 延迟加速有着严重的精度损失
>
><font color='blue'>本文：</font>
>
>我们提出了一个基于空间的动态CNN加速框架Neulens 适用于移动平台和边缘平台
>
>①设计了新的动态推理机制，组装具有区域感知的卷积 超级网
>
>基于空间冗余和信道分割  它剥离了CNN模型内的冗余操作。
>
>在ARAC超网中，CNN推理流被划分为多个独立的微流 ，并且每个的计算成本都可以根据 其平铺输入内容和应用程序需求 进行自动调整。
>
>这些微流可以作为单一模型加载到gpu等硬件中。因此它的操作减少，可以被理解为延迟加速，它是与硬件级的加速相兼容。
>
>推理的精确值可以被很好的维持通过   识别图像上的关键区域   和 在原分辨率使用大量微流来处理他们
>
>NeuLens优于其他方法，在相同的精度下减少高达58%的延迟 以及 准确度提高了高达67.9% 在相同的延迟/内存约束下。
>
>**CCS CONCEPTS**
>
>**Computing methodologies** → *Neural networks*; • **Human**
>
>**centered computing** → *Ubiquitous and mobile computing*.
>
>

1、介绍

>与计算机视觉相关的任务通常需要大量的计算资源,许多研究集中于降低CNN推理的计算成本。一些工作提出了轻量级的网络架构，如MobileNets， CondenseNet , ShuffleNets [51, 93], and EfficientNet [71].其他的研究通过剪枝[42,44,49,50]或量化来压缩现有的网络
>
>最近的工作提出了各种方法，允许动态计算成本调整CNN推理。受人类视觉启发，只有有限的视觉场景可以被视觉系统处理。
>
>最近的工作深入研究了基于**输入空间信息**来降低计算成本的前景，通过提出专门的网络架构 or通过设计与一般CNN架构兼容的计算流
>
>在视频流媒体和分析学，感兴趣区域（RoIs）交叉框架跟踪(Edge-Assisted [47] and Elf [92]) 或者通过低分辨率检测(DDS [9])
>
>通过基于RoI的编码，框架传输数据的大小显著降低了。

<img src="C:\Users\19392\Desktop\myfile\AI_compiler\移动端联邦学习\NeuLens Spatial-based Dynamic Acceleration of Convolutional  Neural Networks on Edge.assets\image-20221201163112673.png" alt="image-20221201163112673" style="zoom: 67%;" />

(2)本文：

>在本文中，我们提出了一个自适应的框架,NeuLens,用于在移动设备和边缘设备上的动态CNN推理加速。首先，我们设计了一种新的动态机构,assemble聚集区域感知卷积（ARAC）超级网,这有效地降低了推理成本，且精度损失较小。ARAC 超网是一个空间分割网络集成。它根据图像与最终预测的相关性，自适应地选择不同大小的子网络。
>
>此外，我们设计了一个轻量级的在线控制器，DEMUX (§5),在实际应用中，基于服务水平目标（SLOs），动态调整每个平铺的子网络选择和超级网络的配置
>
>最后，我们综合评估了 ARAC supernet在不同的移动/边缘平台和各种应用 applications中
>
>根据我们的评估，ARAC supernet在相同的延迟/内存约束下，与最先进的（SOTA）动态推理方法相比，实现了高达67.9%的精度提高。在具有相同的推理延迟，比SOTA模型压缩技术的精度高出1.23×
>
>除此之外，将ARAC超网应用于连续目标检测系统中，比SOTA技术提高7.7×



(3)我们总结了本文的贡献如下：

>①开发用于移动/边缘计算平台的  新型CNN加速机制
>
>通过利用图像上空间和深度冗余，在CNN中，我们提出了一种加速机制，ARAC supernet。这有效地减少了计算资源的消耗，但精度略有降低。
>
>与现有的加速工作相比，ARAC supernet，在准确性-延迟权衡上实现了帕累托最优。我们强调以下高级技术 在 ARAC supernet中：
>
>- 构建ARAC supernet ，这通常适用于CNN的架构。通过将输入图像分割成块， supernet 利用不同压缩级别的子网络进行分析它们。超级网的输出是连接起来的，输入到CNN模型的剩余层中，去计算最后的结果。这种结构允许supernet减少计算中的空间和深度冗余，不影响原网络的整体工作方案。
>
>- 在ARAC supernet中，每一块内容感知调整计算成本。在ARAC supernet中 设计压缩导向门（A compression guiding gate）用于有效分析每块中的内容，以及分配一个具有适当压缩级别的子网去分析。提出一种标签规则来自动生成压缩导向门的训练集
>
>- 从操作冗余减少到设备上延迟加速的有效转换。 In ARAC supernet，将计算流分为多个独立的微流。基于其输入的内容（一个块）。每个微流在独立分析输入时调整操作量（压缩水平）。当每个微流被加载到设备的计算单元中时（如GPU）就像一个单独的神经网络，其操作减少直接转换为延迟加速。
>
>  
>
>②设计一种针对移动/边缘设备的轻量级slo感知控制器，以适应有限的计算预算
>
>我们设计了一个在线轻量级控制器，DEMUX，基于用户的SLO，使用可忽略的在手机或移动设备上的开销来调整调整ARAC supernet.
>
>在一个ARAC supernet 参数中，给了专门的操作， DEMUX自适应地选择最优的参数集，并在用户的SLOs范围内保持较高的精度。
>
>
>
>③实现了ARAC supernet 并 在不同的移动/边缘计算平台和各种视觉应用程序上执行了性能评估
>
>我们从几个方面对ARAC超网的性能进行了综合评价。并证明了其在提高移动/边缘设备上的cnn相关应用程序的整体性能方面的有效性。我们突出我们的评估结果如下：
>
>- 在手机霍边缘设备上，动态预测和模型压缩比SOTA 技术分别高出67.9%（7.2）和1.23×（7.4）
>- 将SOTA连续目标检测系统的整体性能提高7.7×
>- 在针对混合现实设备的SOTA 3D异议检测系统上，端到端延迟减少了近50%

### 2、背景和动机

#### 2.1空间相关卷积

>正如[15,83]所示，在图像中可能有相当数量的冗余像素,它们是与准确识别无关的。一些工作集中于减少冗余像素的卷积操作。大多工作提出了空间神经结构。紧凑型网络被设计用于减少基于空间冗余的操作。序列网络的设计具有多尺度分辨率。
>
>CBAM [80]设计了一个注意模块，它可以插入CNNs中。其他最近的工作提出，基于空间 冗余的计算流修改，这可以普遍适用于CNN 框架，而不用重新设计一个新的。
>
>GFNet [77， 30] 动态处理序列图像上的裁剪，直到具有足够置信度的预测。
>
>DRNet [95] 使用分辨率预测器预测每个输入图像的最佳分辨率
>
>SAR [20]设计了一种双分支网络架构，其中一分支分析低分辨率输入特征，并在每层中为另一层选择高分辨率细化区域。
>
>与这些研究相比，我们提出了一种新的计算流，ARAC supernet,用来处理空间冗余。通过将输入的图片分块，我们基于他们的内容，在supernet中选择不同的子网络。 ARAC supernet 通常适用于流行的CNN架构。
>
>值得注意的是，SAR，CGNet和ASC是不完全支持深度学习平台的实际加速的，消炎药特殊的硬件和框架支持。
>
>相反，我们的工作可以有效在SOTA是的学习平台上实现，并且实现延迟加速。

#### **2.2 Dynamic Inference**动态推断

>我们将动态推理的SOTA研究根据它们是否为平台和SLO自适应研究 分为两种类型。
>
>**与平台和 SLO 无关**：一些工作主要集中在用动态机制来设计或修改单一的网络。早期存在的拓扑被提出用于CNN（[3]，MSDNet [31]和ZTW [79]）。
>
>CNMM [60] 和 RNP [45] 设计了可以动态修剪的网络
>
>Skipnet [76] 和 BlockDrop [81] 根据输入跳过 ResNet 中的块。
>
>ConvNet-AIG [73] 根据估计的相关性确定是否跳过每一层，
>
>CoDiNet [75]基于交叉图像相似度优化层跳过
>
>LCCL [8] 在特征图中避免计算0，通过预测他们的位置。
>
>其他的工作还开发了用于动态推理的网络集成。
>
> Russian Doll Network[35]通过将较小的子网嵌入较大的子网中来构建嵌套网络
>
>HNE [61]设计了一个层次化的神经集成，允许对分支数进行调整。
>
>Slimmable networks 根据不同输入，调整层中的过滤器数量
>
>CoE [94]汇集了由数据集中互斥子集训练的网络集合。
>
>CondConv [86] 构建具有专家混合的子网络，并为每个输入选择它们的组合。
>
>整体而言，这些工作的重点是训练优化和结构修改。他们的设计是手动的，没有考虑到对平台和SLOs的适应性
>
>
>
>**平台和slo相适应的**：相比之下，一些工作集中于在SLOs的移动设备和边缘设备上的，系统级性能的优化。一些工作为一般的cnn设计自适应框架。
>
> NestDNN在一个紧凑的多容量模型 中动态实现资源精度权衡
>
>ReForm [85]提出了一种基于ADMM算法的资源感知DNN重构框架
>
>DMS [38]通过自适应剪枝来控制推理的资源需求。
>
>PatDNN [53]设计了一个基于核模式剪枝的高效DNN框架。
>
>LegoDNN [19]通过切换重新训练的后代块来动态缩放DNNs
>
>其他工作通过  处理视频分析等特定应用程序中的特性   来开发自适应框架 和 实时（视频）目标检测 (AdaVP [48], ApproxDet [84], Remix [37], and [22]).
>
> 
>
>我们：
>
>与他们的工作相比，我们ARAC supernet 实现了一个动态机制 ， 基于空间冗余 通过 将输入分块，并使用不同的压缩子网络来处理它们 。
>
>此外,通过设计用于在线设备推理的sloo自适应在线控制器，我们将ARAC supernet集成到移动系统和边缘系统中
>
>换句话说，我们的工作还解决了对平台和slo的适应性问题。



#### **2.3 Observations** 观察结果

>ARAC supernet和SOTA动态模型的  精度-延迟权衡  如图1所示。
>
>基于空间的工作（MSGFNet [30]，DRNet [95]，MSDNet [31]，CGNet [27,28]，和SAR [20]）和与空间无关的工作（LegoDNN [19]，DS-Net [43]，HNE [61]）都包括在比较中。
>
>在相同的推理延迟，ARAC supernet 的准确率比SOTA方法高出1.22%到2.07%。在具有相同精度，ARAC supernet  减少了19.4%到47.9%的时间在每次推理中。
>
>基于空间冗余的大部分SOTA方法并没有实现实际的加速。其根本原因是 他们在冗余部分上的分割操作，是与图层之间的中间映射不兼容。
>
>例如，SAR[20]（即SOTA像素级动态网络）从每个层的输入特征中选择一组提取的图案，并只对它们运行操作。这些图案是不规则的，每层都会生成不同的patterns ，使得在GPU上计算效率不太好
>
>相比之下, ARAC supernet
>
>
>
>
>
>